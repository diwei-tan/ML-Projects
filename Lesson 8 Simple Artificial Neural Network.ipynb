{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8 Simple Artificial Neural Network\n",
    "\n",
    "Setup\n",
    "\n",
    "~~~\n",
    "conda install seaborn numpy sklearn keras tensorflow\n",
    "~~~\n",
    "\n",
    "Adapted from https://github.com/fastforwardlabs/keras-hello-world\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Iris data\n",
    "\n",
    "We load it from seaborn so that we can load this notebook in Google Colaboratory easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Munge and split the data for training and testing\n",
    "\n",
    "First we need to pull the raw data out of the `iris` dataframe. We'll hold the petal and sepal data in an array `X` and the species labels in a corresponding array `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.values[:, :4]\n",
    "y = iris.values[:, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is standard in supervised machine learning, we'll train with some of the data, and measure the performance of our model with the remainder. This is simple to do by hand, but is also built into scikit-learn as the `train_test_split()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.5, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a scikit-learn classifier\n",
    "\n",
    "We'll train a logisitic regression classifier. Doing this, with built-in hyper-paramter cross validation, is one line in scikit-learn. Like all scikit-learn `Estimator` objects, a `LogisticRegressionCV` classifier has a `.fit()` method that takes care of the gory numerical details of learning model parameters that best fit the training data. So that method is all we need to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=3, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=100, multi_class='ovr', n_jobs=None, penalty='l2',\n",
       "                     random_state=None, refit=True, scoring=None,\n",
       "                     solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegressionCV(cv=3, multi_class='ovr')\n",
    "lr.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess the classifier using accuracy\n",
    "\n",
    "Now we can measure the fraction of of the test set the trained classifer classifies correctly (i.e., accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.83\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {:.2f}\".format(lr.score(test_X, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now do something very similar with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_object_array(arr):\n",
    "    '''One hot encode a numpy array of objects (e.g. strings)'''\n",
    "    uniques, ids = np.unique(arr, return_inverse=True)\n",
    "    return np_utils.to_categorical(ids, len(uniques))\n",
    "\n",
    "train_y_ohe = one_hot_encode_object_array(train_y)\n",
    "test_y_ohe = one_hot_encode_object_array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'virginica' 'versicolor' 'setosa' 'versicolor' 'virginica'\n",
      " 'versicolor' 'setosa' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'setosa' 'setosa' 'virginica' 'virginica' 'setosa' 'virginica' 'setosa'\n",
      " 'virginica' 'virginica' 'setosa' 'setosa' 'virginica' 'setosa' 'setosa'\n",
      " 'setosa' 'versicolor' 'virginica' 'virginica' 'setosa' 'setosa' 'setosa'\n",
      " 'versicolor' 'versicolor' 'setosa' 'setosa' 'versicolor' 'setosa'\n",
      " 'virginica' 'versicolor' 'virginica' 'versicolor' 'setosa' 'virginica'\n",
      " 'setosa' 'virginica' 'setosa' 'setosa' 'virginica' 'setosa' 'virginica'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'virginica' 'virginica'\n",
      " 'versicolor' 'versicolor' 'setosa' 'versicolor' 'virginica' 'virginica'\n",
      " 'setosa' 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'setosa'\n",
      " 'setosa' 'setosa' 'virginica' 'versicolor' 'virginica' 'setosa']\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_y)\n",
    "print(train_y_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the neural network model\n",
    "\n",
    "Aside from the data munging required in this particular case, the most signifcant and important difference with working with Keras is that you have to specify the structure of the model before you can instantiate and use it.\n",
    "\n",
    "In scikit-learn, the models are off-the-shelf. But Keras is a neural network library. As such, while the number of features/classes in your data provide constraints, you can determine all the other aspects of model structure: number of layers, size of layers, the nature of the connections between the layers, etc. (And if that didn't make sense, Keras is a great way to experiment with it!)\n",
    "\n",
    "The upshot of this freedom is that instaniating a minimal classifier involves a bit more work than the one line required by scikit-learn.\n",
    "\n",
    "In our case, we'll build an extremely simple network. Two of the choices are made for us by the data. We have four features and three classes, so the input layer must have four units, and the output layer must have three units. We only have to define the hidden layers. We're only going to have one hidden layer for this project, and we'll give it 16 units. From the point of view of a GPU, 16 is a round number! You'll see a lot of powers of 2 when you work with neural networks.\n",
    "\n",
    "We're going to define our model in the most common way: as a sequential stack of layers. The alternative is as a computational graph, but we're going to stick to `Sequential()` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two lines define the size input layer (input_shape=(4,), and the size and activation function of the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yungh\\Anaconda3\\envs\\sa48\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(16, input_shape=(4,)))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the next line defines the size and activation function of the ouput layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we specify the optimization strategy and the loss function to optimize. We also instruct the model to calculate accuracy as it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 131\n",
      "Trainable params: 131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you figure out how the param# get calculated?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"377pt\" viewBox=\"0.00 0.00 289.00 377.00\" width=\"289pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 373)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-373 285,-373 285,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1826861449736 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1826861449736</title>\n",
       "<polygon fill=\"none\" points=\"22,-249.5 22,-295.5 259,-295.5 259,-249.5 22,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-268.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"126,-249.5 126,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"154\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"126,-272.5 182,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"154\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"182,-249.5 182,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.5\" y=\"-280.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"182,-272.5 259,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.5\" y=\"-257.3\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 1826861449848 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1826861449848</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 281,-212.5 281,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-185.8\">activation_1: Activation</text>\n",
       "<polyline fill=\"none\" points=\"148,-166.5 148,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"148,-189.5 204,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"204,-166.5 204,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-197.3\">(None, 16)</text>\n",
       "<polyline fill=\"none\" points=\"204,-189.5 281,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-174.3\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 1826861449736&#45;&gt;1826861449848 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1826861449736-&gt;1826861449848</title>\n",
       "<path d=\"M140.5,-249.366C140.5,-241.152 140.5,-231.658 140.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"144,-222.607 140.5,-212.607 137,-222.607 144,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1826864248256 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1826864248256</title>\n",
       "<polygon fill=\"none\" points=\"22,-83.5 22,-129.5 259,-129.5 259,-83.5 22,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-102.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"126,-83.5 126,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"154\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"126,-106.5 182,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"154\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"182,-83.5 182,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.5\" y=\"-114.3\">(None, 16)</text>\n",
       "<polyline fill=\"none\" points=\"182,-106.5 259,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.5\" y=\"-91.3\">(None, 3)</text>\n",
       "</g>\n",
       "<!-- 1826861449848&#45;&gt;1826864248256 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1826861449848-&gt;1826864248256</title>\n",
       "<path d=\"M140.5,-166.366C140.5,-158.152 140.5,-148.658 140.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"144,-139.607 140.5,-129.607 137,-139.607 144,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1826864247920 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1826864247920</title>\n",
       "<polygon fill=\"none\" points=\"3.5,-0.5 3.5,-46.5 277.5,-46.5 277.5,-0.5 3.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"77.5\" y=\"-19.8\">activation_2: Activation</text>\n",
       "<polyline fill=\"none\" points=\"151.5,-0.5 151.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"179.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"151.5,-23.5 207.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"179.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"207.5,-0.5 207.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-31.3\">(None, 3)</text>\n",
       "<polyline fill=\"none\" points=\"207.5,-23.5 277.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-8.3\">(None, 3)</text>\n",
       "</g>\n",
       "<!-- 1826864248256&#45;&gt;1826864247920 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>1826864248256-&gt;1826864247920</title>\n",
       "<path d=\"M140.5,-83.3664C140.5,-75.1516 140.5,-65.6579 140.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"144,-56.6068 140.5,-46.6068 137,-56.6069 144,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1826864083240 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1826864083240</title>\n",
       "<polygon fill=\"none\" points=\"88.5,-332.5 88.5,-368.5 192.5,-368.5 192.5,-332.5 88.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"140.5\" y=\"-346.8\">1826864083240</text>\n",
       "</g>\n",
       "<!-- 1826864083240&#45;&gt;1826861449736 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1826864083240-&gt;1826861449736</title>\n",
       "<path d=\"M140.5,-332.254C140.5,-324.363 140.5,-314.749 140.5,-305.602\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"144,-305.591 140.5,-295.591 137,-305.591 144,-305.591\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the neural network classifier\n",
    "\n",
    "Now we've defined the structure model and compiled it, we have an object whose API is almost identical to a classifier in scikit-learn. In particular, it has `.fit()` and `.predict()` methods. Let's `fit`.\n",
    "\n",
    "Training neural networks often involves the concept of \"minibatching\", which means showing the network a subset of the data, adjusting the weights, and then showing it another subset of the data. When the network has seen all the data once, that's called an \"epoch\". Tuning the minibatch/epoch strategy is a somewhat problem-specific issue, but in this case we'll just use a minibatch of 1. That makes it effectively good old stochastic gradient descent, i.e. the data is shown to the network one flower at a time, and the weights adjusted immediately.\n",
    "\n",
    "Feel free to rerun this cell with `verbose=0` removed. Note that if you want to experiment with the minibatch/epoch strategy you should rerun the previous cell that ran `model.compile()` to reinitialize the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yungh\\Anaconda3\\envs\\sa48\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "75/75 [==============================] - 1s 13ms/step - loss: 1.4170 - acc: 0.2667\n",
      "Epoch 2/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.1487 - acc: 0.2800\n",
      "Epoch 3/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0578 - acc: 0.6400\n",
      "Epoch 4/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0011 - acc: 0.7333\n",
      "Epoch 5/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.9543 - acc: 0.7333\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.9127 - acc: 0.7333\n",
      "Epoch 7/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.8722 - acc: 0.7333\n",
      "Epoch 8/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.8348 - acc: 0.7333\n",
      "Epoch 9/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.8022 - acc: 0.7333\n",
      "Epoch 10/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.7598 - acc: 0.7333\n",
      "Epoch 11/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.7311 - acc: 0.7333\n",
      "Epoch 12/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6951 - acc: 0.7333\n",
      "Epoch 13/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6657 - acc: 0.7333\n",
      "Epoch 14/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6347 - acc: 0.7467\n",
      "Epoch 15/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6106 - acc: 0.7600\n",
      "Epoch 16/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5899 - acc: 0.8400\n",
      "Epoch 17/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5661 - acc: 0.7333\n",
      "Epoch 18/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5444 - acc: 0.7867\n",
      "Epoch 19/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5249 - acc: 0.8000\n",
      "Epoch 20/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5103 - acc: 0.7867\n",
      "Epoch 21/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4941 - acc: 0.8267\n",
      "Epoch 22/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4802 - acc: 0.9067\n",
      "Epoch 23/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4664 - acc: 0.7867\n",
      "Epoch 24/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4528 - acc: 0.8133\n",
      "Epoch 25/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4397 - acc: 0.9600\n",
      "Epoch 26/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4290 - acc: 0.8533\n",
      "Epoch 27/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4203 - acc: 0.9200\n",
      "Epoch 28/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4093 - acc: 0.9200\n",
      "Epoch 29/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3997 - acc: 0.9200\n",
      "Epoch 30/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3923 - acc: 0.8400\n",
      "Epoch 31/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3893 - acc: 0.9600\n",
      "Epoch 32/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3790 - acc: 0.9200\n",
      "Epoch 33/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3676 - acc: 0.9333\n",
      "Epoch 34/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3623 - acc: 0.9600\n",
      "Epoch 35/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3539 - acc: 0.9200\n",
      "Epoch 36/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3460 - acc: 0.9333\n",
      "Epoch 37/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3388 - acc: 0.9600\n",
      "Epoch 38/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3316 - acc: 0.9467\n",
      "Epoch 39/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3261 - acc: 0.9600\n",
      "Epoch 40/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3212 - acc: 0.9467\n",
      "Epoch 41/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3129 - acc: 0.9600\n",
      "Epoch 42/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3066 - acc: 0.9467\n",
      "Epoch 43/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3010 - acc: 0.9600\n",
      "Epoch 44/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2971 - acc: 0.9467\n",
      "Epoch 45/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2890 - acc: 0.9467\n",
      "Epoch 46/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2872 - acc: 0.9600\n",
      "Epoch 47/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2792 - acc: 0.9467\n",
      "Epoch 48/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2752 - acc: 0.9867\n",
      "Epoch 49/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2722 - acc: 0.9600\n",
      "Epoch 50/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2647 - acc: 0.9600\n",
      "Epoch 51/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2642 - acc: 0.9467\n",
      "Epoch 52/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2580 - acc: 0.9600\n",
      "Epoch 53/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2494 - acc: 0.9467\n",
      "Epoch 54/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2461 - acc: 0.9467\n",
      "Epoch 55/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2404 - acc: 0.9467\n",
      "Epoch 56/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2354 - acc: 0.9600\n",
      "Epoch 57/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2366 - acc: 0.9867\n",
      "Epoch 58/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2289 - acc: 0.9467\n",
      "Epoch 59/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2222 - acc: 0.9467\n",
      "Epoch 60/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2204 - acc: 0.9867\n",
      "Epoch 61/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2142 - acc: 0.9600\n",
      "Epoch 62/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2117 - acc: 0.9467\n",
      "Epoch 63/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2085 - acc: 0.9600\n",
      "Epoch 64/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2044 - acc: 0.9733\n",
      "Epoch 65/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2039 - acc: 0.9733\n",
      "Epoch 66/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1956 - acc: 0.9600\n",
      "Epoch 67/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1923 - acc: 0.9600\n",
      "Epoch 68/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1880 - acc: 0.9600\n",
      "Epoch 69/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1852 - acc: 0.9733\n",
      "Epoch 70/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1829 - acc: 0.9600\n",
      "Epoch 71/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1786 - acc: 0.9600\n",
      "Epoch 72/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1766 - acc: 0.9733\n",
      "Epoch 73/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1731 - acc: 0.9600\n",
      "Epoch 74/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1716 - acc: 0.9600\n",
      "Epoch 75/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1658 - acc: 0.9600\n",
      "Epoch 76/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1637 - acc: 0.9733\n",
      "Epoch 77/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1608 - acc: 0.9733\n",
      "Epoch 78/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1596 - acc: 0.9600\n",
      "Epoch 79/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1570 - acc: 0.9867\n",
      "Epoch 80/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1538 - acc: 0.9733\n",
      "Epoch 81/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1501 - acc: 0.9867\n",
      "Epoch 82/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1475 - acc: 0.9600\n",
      "Epoch 83/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1458 - acc: 0.9733\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1455 - acc: 0.9867\n",
      "Epoch 85/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1419 - acc: 0.9600\n",
      "Epoch 86/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1406 - acc: 0.9867\n",
      "Epoch 87/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1366 - acc: 0.9733\n",
      "Epoch 88/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1403 - acc: 0.9600\n",
      "Epoch 89/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1355 - acc: 0.9733\n",
      "Epoch 90/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1324 - acc: 0.9600\n",
      "Epoch 91/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1314 - acc: 0.9867\n",
      "Epoch 92/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1309 - acc: 0.9733\n",
      "Epoch 93/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1254 - acc: 0.9733\n",
      "Epoch 94/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1242 - acc: 0.9867\n",
      "Epoch 95/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1235 - acc: 0.9600\n",
      "Epoch 96/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1229 - acc: 0.9733\n",
      "Epoch 97/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1214 - acc: 0.9733\n",
      "Epoch 98/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1221 - acc: 0.9600\n",
      "Epoch 99/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1164 - acc: 0.9867\n",
      "Epoch 100/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1200 - acc: 0.9600\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_X, train_y_ohe, epochs=100, batch_size=1, verbose=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For basic use, the only syntactic API difference between a compiled keras model and a scikit-learn classifier is that Keras's equivalent of the scikit-learn `.score()` method is called `.evaluate()`. \n",
    "\n",
    "`evaluate()` returns the loss function and any other metrics we asked for when we compiled the model. In our case, we asked for `accuracy`, which we can compare to the accuracy we got from the `.score()` method of our scikit-learn `LogisticRegressionCV` classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.97\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_X, test_y_ohe, verbose=0)\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the test accuracy of the neural network model is better than that of the simple logistic regression classifier.\n",
    "\n",
    "This is reassuring but not surprising. Even our very simple neural network has the flexibility to learn much more complicated classification surfaces than logisitic regression, so of course it does better than logisitic regression.\n",
    "\n",
    "And it does hint at one of the dangers of neural networks: overfitting. We've been careful here to hold out a test set and measure performance with that, but it's a small set, and 99% accuracy seems awfully high to me, so I wouldn't be surprised if there was some overfitting going on. You could work on that by adding dropout (which is built in to Keras). That's the neural network equivalent of the regularization our LogisticRegression classifier uses.\n",
    "\n",
    "But we'll stop here, because a neural network model is overkill for this problem, and worrying about accuracy is not the point of what I've shown you. The point is that using a batteries-included, high-level library like Keras means that we need to write only a little bit more code to build, train, and apply a neural network model than a traditional model.\n",
    "\n",
    "# What next?\n",
    "\n",
    "We built an extremely simple feed-forward network model. To experiment more, load the [MNIST database of handwritten digits](http://keras.io/datasets/) and see if you can beat a standard scikit-learn classifier. Unlike the Iris dataset, this is a situation where the power and relative complexity of neural networks is justified. Try it yourself first, but if you get stuck, take a look at [this notebook](https://github.com/wxs/keras-mnist-tutorial/blob/master/MNIST%20in%20Keras.ipynb).\n",
    "\n",
    "Technical and engineering tutorials for your favourite language or library are easy to find, but if you're interested in some of the more conceptual and mathematical background, take a look at:\n",
    " - Michael Nielsen's online textbook \"[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)\" (especially chapters 1, 2 and 3)\n",
    " - Weeks 4 and 5 of Andrew Ng's Coursera course \"[Machine Learning](https://www.coursera.org/learn/machine-learning)\" (you'll have to sign up for the course to access these materials)\n",
    " - \"[Deep Learning](http://www.nature.com/nature/journal/v521/n7553/abs/nature14539.html)\" by Yan Le Cun et al. in Nature (2015) (you can find a [PDF of this article on Google Scholar](https://scholar.google.com/scholar?cluster=5362332738201102290))\n",
    " - Chris Olah's [wonderful essays](http://colah.github.io/), particularly the ones on [back propagation](http://colah.github.io/posts/2015-08-Backprop/) (the algorithm with which neural networks are trained) and [recurrent neural networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "Speaking of recurrent neural networks, Keras also has layers that allow you to build models with:\n",
    "\n",
    " - convolutional layers, which give state-of-the-art results for computer vsion problems;\n",
    " - recurrent layers, which are particularly well suited to modelling language and other sequence data.\n",
    " \n",
    "In fact, one key strength of neural networks (along with sheer predictive power) is their composability. Using a high-level library like Keras, it takes only a few seconds of work to create a very different network. Models can be built up like Lego. Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try to run a Neural Network Classifier on some data of your choice (perhaps your CA) and observe its performance. Is it better or worse than the traditional algorithms?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Lesson - Saving Our Model\n",
    "\n",
    "So far we are limited to our Jupyter Notebook environment. If we were to make our model useful, we need to be able to expose the learning capability to other systems through services/method call. In order to do that, we can save our model after our training. \n",
    "\n",
    "In this notebook, we have trained two models that can predict IRIS dataset using Logistic Regression and Neural Network. Let's see if we can expose those as a service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our Logistic Regression model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lr_iris.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(lr, 'lr_iris.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Logistic Regression model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.83\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "loaded_lr = clf = load('lr_iris.joblib') \n",
    "\n",
    "print(\"Accuracy = {:.2f}\".format(loaded_lr.score(test_X, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virginica\n"
     ]
    }
   ],
   "source": [
    "pred_y = loaded_lr.predict([[5.8, 2.8, 5.1, 2.4]])\n",
    "print (pred_y[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our Keras model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"iris_nn.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"iris_nn.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Keras model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('iris_nn.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"iris_nn.h5\")\n",
    "print(\"Loaded model from disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.97\n"
     ]
    }
   ],
   "source": [
    "loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "loss, accuracy = loaded_model.evaluate(test_X, test_y_ohe, verbose=0)\n",
    "\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "nn_y = loaded_model.predict(np.array([[5.8, 2.8, 5.1, 2.4],]))\n",
    "\n",
    "print(np.round(nn_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
