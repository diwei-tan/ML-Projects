{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9 Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Run this command from an Anaconda prompt:\n",
    "\n",
    "```\n",
    "conda install nltk spacy scikit-learn pandas\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "- Overview\n",
    "- Parsing, Stemming, Lemmatization\n",
    "- Named Entity Recognition\n",
    "- Stop Words\n",
    "- Frequency Analysis\n",
    "- Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Toolkits\n",
    "\n",
    "### NLTK: NLP toolkit\n",
    "\n",
    "Book: http://www.nltk.org/book/\n",
    "\n",
    "Wiki: https://github.com/nltk/nltk/wiki\n",
    "\n",
    "Corpus: http://www.nltk.org/nltk_data/\n",
    "\n",
    "### spaCy: another NLP toolkit\n",
    "\n",
    "Simpler to use than NLTK (but usually fewer knobs)\n",
    "\n",
    "API: https://spacy.io/api/\n",
    "\n",
    "Models: https://spacy.io/usage/models\n",
    "\n",
    "Tutorial: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# What is Text Processing?\n",
    "\n",
    "- A sub-field of Natural Language Processing (NLP)\n",
    "- Natural Language Processing is ...\n",
    " - Teaching machines to understand and produce language (text, speech)\n",
    " - A combination of computer science and computational linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Text Processing Tasks\n",
    "\n",
    "- Word categorization and tagging: part of speech, type of entity\n",
    "- Semantic Analysis: finding meanings of documents\n",
    "- Topic Modeling: finding topics from documents\n",
    "- Document similarity: comparing if two documents are semantically similar\n",
    "- etc.\n",
    "\n",
    "Note: Speech is text processing + acoustic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parsing, Stemming & Lemmatization\n",
    "\n",
    "- Tokenization: splitting text into words\n",
    "- Sentence boundary detection: splitting text into sentences\n",
    "- Stemming: finding word stems\n",
    "   - stating => state, reference => refer\n",
    "- Lemmatization: finding the base form of words\n",
    "   - was => be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "- Segmenting text into words, punctuation, etc.\n",
    "- Rule-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in c:\\users\\yungh\\anaconda3\\envs\\sa48\\lib\\site-packages (2.0.0)\n",
      "\n",
      "    Error: Couldn't link model to 'en_core_web_sm'\n",
      "    Creating a symlink in spacy/data failed. Make sure you have the required\n",
      "    permissions and try re-running the command as admin, or use a\n",
      "    virtualenv. You can still import the model as a module and call its\n",
      "    load() method, or create the symlink manually.\n",
      "\n",
      "    C:\\Users\\yungh\\Anaconda3\\envs\\sa48\\lib\\site-packages\\en_core_web_sm -->\n",
      "    C:\\Users\\yungh\\Anaconda3\\envs\\sa48\\lib\\site-packages\\spacy\\data\\en_core_web_sm\n",
      "\n",
      "\n",
      "    Creating a shortcut link for 'en' didn't work (maybe you don't have\n",
      "    admin permissions?), but you can still load the model via its full\n",
      "    package name: nlp = spacy.load('{name}')\n",
      "    Download successful but linking failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the English model\n",
    "# You can find other models here: https://spacy.io/models/en\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = u\"This is a test. A quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is a test.\n",
      "\n",
      "A quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# sentence tokenizer\n",
    "for sent in doc.sents:\n",
    "    print()\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "a\n",
      "test\n",
      ".\n",
      "A\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jumps\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# word tokenizer\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'determiner'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('DET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/api/token\n",
    "\n",
    "https://spacy.io/api/token#attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1870\" height=\"347.0\" style=\"max-width: none; height: 347.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"190\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"190\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"330\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"330\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"470\">test.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"470\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">A</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">quick</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">brown</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1030\">fox</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1030\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1170\">jumps</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1170\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1310\">over</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1310\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1590\">lazy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1590\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1730\">dog.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1730\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,212.0 C70,142.0 180.0,142.0 180.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,214.0 L62,202.0 78,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M350,212.0 C350,142.0 460.0,142.0 460.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M350,214.0 L342,202.0 358,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M210,212.0 C210,72.0 465.0,72.0 465.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M465.0,214.0 L473.0,202.0 457.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M630,212.0 C630,2.0 1030.0,2.0 1030.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M630,214.0 L622,202.0 638,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M770,212.0 C770,72.0 1025.0,72.0 1025.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,214.0 L762,202.0 778,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M910,212.0 C910,142.0 1020.0,142.0 1020.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910,214.0 L902,202.0 918,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M1050,212.0 C1050,142.0 1160.0,142.0 1160.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1050,214.0 L1042,202.0 1058,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M1190,212.0 C1190,142.0 1300.0,142.0 1300.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1300.0,214.0 L1308.0,202.0 1292.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M1470,212.0 C1470,72.0 1725.0,72.0 1725.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,214.0 L1462,202.0 1478,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M1610,212.0 C1610,142.0 1720.0,142.0 1720.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1610,214.0 L1602,202.0 1618,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-10\" stroke-width=\"2px\" d=\"M1330,212.0 C1330,2.0 1730.0,2.0 1730.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-10\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1730.0,214.0 L1738.0,202.0 1722.0,202.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 140})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Tokenization with NLTK\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "nltk.tokenize\n",
    " - sent_tokenize\n",
    " - word_tokenize\n",
    " - wordpunc_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yungh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the Punkt sentence tokenizer\n",
    "# https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "\n",
    "# List of available corpora: http://www.nltk.org/book/ch02.html#tab-corpora\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a test.', 'A quick brown fox jumps over the lazy dog.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# list of sentences\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " '.',\n",
       " 'A',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# flat list of words and punctuations\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is', 'a', 'test', '.'],\n",
       " ['A', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# list of lists\n",
    "[word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'The\", 'time', 'is', 'now', '5.30am', ',', \"'\", 'he', 'said', '.']\n",
      "[\"'\", 'The', 'time', 'is', 'now', '5', '.', '30am', \",'\", 'he', 'said', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text2 = \"'The time is now 5.30am,' he said.\"\n",
    "\n",
    "print(word_tokenize(text2))\n",
    "\n",
    "print(wordpunct_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\yungh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('test', 'NN'), ('.', '.')],\n",
       " [('A', 'DT'),\n",
       "  ('quick', 'JJ'),\n",
       "  ('brown', 'NN'),\n",
       "  ('fox', 'NN'),\n",
       "  ('jumps', 'VBZ'),\n",
       "  ('over', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('lazy', 'JJ'),\n",
       "  ('dog', 'NN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part of speech tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "[nltk.pos_tag(word) for word in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjective'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Twitter-aware tokenizer\n",
    "\n",
    "`nltk.tokenize.TweetTokenizer`\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "\n",
    "tknzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweet = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
    "\n",
    "tknzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming vs. Lemmatization\n",
    "\n",
    "- Stemming uses rule-based heuristics\n",
    "  - ponies => poni\n",
    "  - Quicker, but less precision\n",
    "- Lemmatization uses vocabulary and morphological analysis\n",
    "  - ponies => pony\n",
    "  - For English, not much improvement over stemming because context of word use is more important\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Porter Stemmer\n",
    "\n",
    "- 5 sequential phases of word reductions\n",
    "- Applies rules such as \"sses -> ss\", \"ies => i\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stemming & Lemmatization with spaCy\n",
    "\n",
    "`spacy.lemmatizer.Lemmatizer`\n",
    "\n",
    "https://spacy.io/api/lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this']\n",
      "['be']\n",
      "['a']\n",
      "['test']\n",
      "['.']\n",
      "['a']\n",
      "['quick']\n",
      "['brown']\n",
      "['fox']\n",
      "['jump']\n",
      "['over']\n",
      "['the']\n",
      "['lazy']\n",
      "['dog']\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(lemmatizer(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stemming & Lemmatization with NLTK\n",
    "\n",
    "`nltk.stem`\n",
    "- `PorterStemmer`\n",
    "- `WordNetLemmatizer`\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thi\n",
      "is\n",
      "a\n",
      "test\n",
      ".\n",
      "A\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jump\n",
      "over\n",
      "the\n",
      "lazi\n",
      "dog\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yungh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "a\n",
      "test\n",
      ".\n",
      "A\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jump\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "- Find and classify entities within text\n",
    "  - Persons\n",
    "  - Organizations\n",
    "  - Locations\n",
    "  - Time expressions\n",
    "  - Quantities\n",
    "  - Phone numbers\n",
    "  - etc\n",
    "  \n",
    "- Grammar-based models, trained classifiers\n",
    "\n",
    "- Can be corpus-dependent, see https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Named Entity Recognition with spaCy\n",
    "\n",
    "https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flight 224 == PRODUCT == 0 == 10\n",
      "Frankfurt == GPE == 37 == 46\n",
      "4pm == TIME == 50 == 53\n",
      "July 5th, 2018 == DATE == 54 == 68\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text3 = u\"Flight 224 is scheduled to arrive in Frankfurt at 4pm July 5th, 2018.\"\n",
    "doc = nlp(text3)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, '==', entity.label_,  '==', entity.start_char,  '==', entity.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'July 5th, 2018'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3[54:68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NORP')\n",
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Flight 224\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " is scheduled to arrive in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Frankfurt\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " at \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    4pm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    July 5th, 2018\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Named Entity Recognition with NLTK\n",
    "\n",
    "```\n",
    "nltk.ne_chunk()\n",
    "```\n",
    "\n",
    "https://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\yungh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\yungh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Tree('S', [('Flight', 'NNP'), ('224', 'CD'), ('is', 'VBZ'), ('scheduled', 'VBN'), ('to', 'TO'), ('arrive', 'VB'), ('in', 'IN'), Tree('GPE', [('Frankfurt', 'NNP')]), ('at', 'IN'), ('4pm', 'CD'), ('July', 'NNP'), ('5th', 'CD'), (',', ','), ('2018', 'CD'), ('.', '.')])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text3)\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Input to ne_chunk needs to be a part-of-speech tagged word\n",
    "sentences_pos_tagged = [nltk.pos_tag(word) for word in sentences]\n",
    "\n",
    "[nltk.ne_chunk(word_pos) for word_pos in sentences_pos_tagged]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Stop words\n",
    "\n",
    "Stop words are high-frequency words that don't contribute much lexical content:\n",
    "\n",
    "- the\n",
    "- a\n",
    "- to\n",
    "\n",
    "NLP libraries usually include a corpus of stop words.\n",
    "\n",
    "Stop word lists:\n",
    "- http://www.nltk.org/book/ch02.html#stopwords_index_term\n",
    "- https://www.semantikoz.com/blog/free-stop-word-lists-in-23-languages/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stop words with spaCy\n",
    "\n",
    "`spacy.lang.en.stop_words`\n",
    "\n",
    "`token.is_stop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'ab',\n",
       " 'aber',\n",
       " 'ach',\n",
       " 'acht',\n",
       " 'achte',\n",
       " 'achten',\n",
       " 'achter',\n",
       " 'achtes',\n",
       " 'ag',\n",
       " 'alle',\n",
       " 'allein',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'allerdings',\n",
       " 'alles',\n",
       " 'allgemeinen',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'andere',\n",
       " 'anderen',\n",
       " 'andern',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'ausser',\n",
       " 'ausserdem',\n",
       " 'außer',\n",
       " 'außerdem',\n",
       " 'bald',\n",
       " 'bei',\n",
       " 'beide',\n",
       " 'beiden',\n",
       " 'beim',\n",
       " 'beispiel',\n",
       " 'bekannt',\n",
       " 'bereits',\n",
       " 'besonders',\n",
       " 'besser',\n",
       " 'besten',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bisher',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'dabei',\n",
       " 'dadurch',\n",
       " 'dafür',\n",
       " 'dagegen',\n",
       " 'daher',\n",
       " 'dahin',\n",
       " 'dahinter',\n",
       " 'damals',\n",
       " 'damit',\n",
       " 'danach',\n",
       " 'daneben',\n",
       " 'dank',\n",
       " 'dann',\n",
       " 'daran',\n",
       " 'darauf',\n",
       " 'daraus',\n",
       " 'darf',\n",
       " 'darfst',\n",
       " 'darin',\n",
       " 'darum',\n",
       " 'darunter',\n",
       " 'darüber',\n",
       " 'das',\n",
       " 'dasein',\n",
       " 'daselbst',\n",
       " 'dass',\n",
       " 'dasselbe',\n",
       " 'davon',\n",
       " 'davor',\n",
       " 'dazu',\n",
       " 'dazwischen',\n",
       " 'daß',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deiner',\n",
       " 'dem',\n",
       " 'dementsprechend',\n",
       " 'demgegenüber',\n",
       " 'demgemäss',\n",
       " 'demgemäß',\n",
       " 'demselben',\n",
       " 'demzufolge',\n",
       " 'den',\n",
       " 'denen',\n",
       " 'denn',\n",
       " 'denselben',\n",
       " 'der',\n",
       " 'deren',\n",
       " 'derjenige',\n",
       " 'derjenigen',\n",
       " 'dermassen',\n",
       " 'dermaßen',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'des',\n",
       " 'deshalb',\n",
       " 'desselben',\n",
       " 'dessen',\n",
       " 'deswegen',\n",
       " 'dich',\n",
       " 'die',\n",
       " 'diejenige',\n",
       " 'diejenigen',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'dir',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'drei',\n",
       " 'drin',\n",
       " 'dritte',\n",
       " 'dritten',\n",
       " 'dritter',\n",
       " 'drittes',\n",
       " 'du',\n",
       " 'durch',\n",
       " 'durchaus',\n",
       " 'durfte',\n",
       " 'durften',\n",
       " 'dürfen',\n",
       " 'dürft',\n",
       " 'eben',\n",
       " 'ebenso',\n",
       " 'ehrlich',\n",
       " 'eigen',\n",
       " 'eigene',\n",
       " 'eigenen',\n",
       " 'eigener',\n",
       " 'eigenes',\n",
       " 'ein',\n",
       " 'einander',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einigeeinigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'einmaleins',\n",
       " 'elf',\n",
       " 'en',\n",
       " 'ende',\n",
       " 'endlich',\n",
       " 'entweder',\n",
       " 'er',\n",
       " 'erst',\n",
       " 'erste',\n",
       " 'ersten',\n",
       " 'erster',\n",
       " 'erstes',\n",
       " 'es',\n",
       " 'etwa',\n",
       " 'etwas',\n",
       " 'euch',\n",
       " 'früher',\n",
       " 'fünf',\n",
       " 'fünfte',\n",
       " 'fünften',\n",
       " 'fünfter',\n",
       " 'fünftes',\n",
       " 'für',\n",
       " 'gab',\n",
       " 'ganz',\n",
       " 'ganze',\n",
       " 'ganzen',\n",
       " 'ganzer',\n",
       " 'ganzes',\n",
       " 'gar',\n",
       " 'gedurft',\n",
       " 'gegen',\n",
       " 'gegenüber',\n",
       " 'gehabt',\n",
       " 'gehen',\n",
       " 'geht',\n",
       " 'gekannt',\n",
       " 'gekonnt',\n",
       " 'gemacht',\n",
       " 'gemocht',\n",
       " 'gemusst',\n",
       " 'genug',\n",
       " 'gerade',\n",
       " 'gern',\n",
       " 'gesagt',\n",
       " 'geschweige',\n",
       " 'gewesen',\n",
       " 'gewollt',\n",
       " 'geworden',\n",
       " 'gibt',\n",
       " 'ging',\n",
       " 'gleich',\n",
       " 'gott',\n",
       " 'gross',\n",
       " 'grosse',\n",
       " 'grossen',\n",
       " 'grosser',\n",
       " 'grosses',\n",
       " 'groß',\n",
       " 'große',\n",
       " 'großen',\n",
       " 'großer',\n",
       " 'großes',\n",
       " 'gut',\n",
       " 'gute',\n",
       " 'guter',\n",
       " 'gutes',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'habt',\n",
       " 'hast',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'heisst',\n",
       " 'heißt',\n",
       " 'her',\n",
       " 'heute',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'hoch',\n",
       " 'hätte',\n",
       " 'hätten',\n",
       " 'ich',\n",
       " 'ihm',\n",
       " 'ihn',\n",
       " 'ihnen',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'im',\n",
       " 'immer',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'infolgedessen',\n",
       " 'ins',\n",
       " 'irgend',\n",
       " 'ist',\n",
       " 'ja',\n",
       " 'jahr',\n",
       " 'jahre',\n",
       " 'jahren',\n",
       " 'je',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedermann',\n",
       " 'jedermanns',\n",
       " 'jedoch',\n",
       " 'jemand',\n",
       " 'jemandem',\n",
       " 'jemanden',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kam',\n",
       " 'kann',\n",
       " 'kannst',\n",
       " 'kaum',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'kleine',\n",
       " 'kleinen',\n",
       " 'kleiner',\n",
       " 'kleines',\n",
       " 'kommen',\n",
       " 'kommt',\n",
       " 'konnte',\n",
       " 'konnten',\n",
       " 'kurz',\n",
       " 'können',\n",
       " 'könnt',\n",
       " 'könnte',\n",
       " 'lang',\n",
       " 'lange',\n",
       " 'leicht',\n",
       " 'leider',\n",
       " 'lieber',\n",
       " 'los',\n",
       " 'machen',\n",
       " 'macht',\n",
       " 'machte',\n",
       " 'mag',\n",
       " 'magst',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mehr',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mensch',\n",
       " 'menschen',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'mit',\n",
       " 'mittel',\n",
       " 'mochte',\n",
       " 'mochten',\n",
       " 'morgen',\n",
       " 'muss',\n",
       " 'musst',\n",
       " 'musste',\n",
       " 'mussten',\n",
       " 'muß',\n",
       " 'möchte',\n",
       " 'mögen',\n",
       " 'möglich',\n",
       " 'mögt',\n",
       " 'müssen',\n",
       " 'müsst',\n",
       " 'na',\n",
       " 'nach',\n",
       " 'nachdem',\n",
       " 'nahm',\n",
       " 'natürlich',\n",
       " 'neben',\n",
       " 'nein',\n",
       " 'neue',\n",
       " 'neuen',\n",
       " 'neun',\n",
       " 'neunte',\n",
       " 'neunten',\n",
       " 'neunter',\n",
       " 'neuntes',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'nie',\n",
       " 'niemand',\n",
       " 'niemandem',\n",
       " 'niemanden',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oben',\n",
       " 'oder',\n",
       " 'offen',\n",
       " 'oft',\n",
       " 'ohne',\n",
       " 'recht',\n",
       " 'rechte',\n",
       " 'rechten',\n",
       " 'rechter',\n",
       " 'rechtes',\n",
       " 'richtig',\n",
       " 'rund',\n",
       " 'sagt',\n",
       " 'sagte',\n",
       " 'sah',\n",
       " 'satt',\n",
       " 'schlecht',\n",
       " 'schon',\n",
       " 'sechs',\n",
       " 'sechste',\n",
       " 'sechsten',\n",
       " 'sechster',\n",
       " 'sechstes',\n",
       " 'sehr',\n",
       " 'sei',\n",
       " 'seid',\n",
       " 'seien',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'seit',\n",
       " 'seitdem',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'sieben',\n",
       " 'siebente',\n",
       " 'siebenten',\n",
       " 'siebenter',\n",
       " 'siebentes',\n",
       " 'siebte',\n",
       " 'siebten',\n",
       " 'siebter',\n",
       " 'siebtes',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solang',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollen',\n",
       " 'sollte',\n",
       " 'sollten',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'sowie',\n",
       " 'später',\n",
       " 'statt',\n",
       " 'tag',\n",
       " 'tage',\n",
       " 'tagen',\n",
       " 'tat',\n",
       " 'teil',\n",
       " 'tel',\n",
       " 'trotzdem',\n",
       " 'tun',\n",
       " 'uhr',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unser',\n",
       " 'unsere',\n",
       " 'unserer',\n",
       " 'unter',\n",
       " 'vergangene',\n",
       " 'vergangenen',\n",
       " 'viel',\n",
       " 'viele',\n",
       " 'vielem',\n",
       " 'vielen',\n",
       " 'vielleicht',\n",
       " 'vier',\n",
       " 'vierte',\n",
       " 'vierten',\n",
       " 'vierter',\n",
       " 'viertes',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'wahr',\n",
       " 'wann',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'wart',\n",
       " 'warum',\n",
       " 'was',\n",
       " 'wegen',\n",
       " 'weil',\n",
       " 'weit',\n",
       " 'weiter',\n",
       " 'weitere',\n",
       " 'weiteren',\n",
       " 'weiteres',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wem',\n",
       " 'wen',\n",
       " 'wenig',\n",
       " 'wenige',\n",
       " 'weniger',\n",
       " 'weniges',\n",
       " 'wenigstens',\n",
       " 'wenn',\n",
       " 'wer',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'werdet',\n",
       " 'wessen',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'willst',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirklich',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wohl',\n",
       " 'wollen',\n",
       " 'wollt',\n",
       " 'wollte',\n",
       " 'wollten',\n",
       " 'worden',\n",
       " 'wurde',\n",
       " 'wurden',\n",
       " 'während',\n",
       " 'währenddem',\n",
       " 'währenddessen',\n",
       " 'wäre',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zehn',\n",
       " 'zehnte',\n",
       " 'zehnten',\n",
       " 'zehnter',\n",
       " 'zehntes',\n",
       " 'zeit',\n",
       " 'zu',\n",
       " 'zuerst',\n",
       " 'zugleich',\n",
       " 'zum',\n",
       " 'zunächst',\n",
       " 'zur',\n",
       " 'zurück',\n",
       " 'zusammen',\n",
       " 'zwanzig',\n",
       " 'zwar',\n",
       " 'zwei',\n",
       " 'zweite',\n",
       " 'zweiten',\n",
       " 'zweiter',\n",
       " 'zweites',\n",
       " 'zwischen',\n",
       " 'á',\n",
       " 'über',\n",
       " 'überhaupt',\n",
       " 'übrigens'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deutsch\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flight False\n",
      "224 False\n",
      "is True\n",
      "scheduled False\n",
      "to True\n",
      "arrive False\n",
      "in True\n",
      "Frankfurt False\n",
      "at True\n",
      "4 False\n",
      "pm False\n",
      "July False\n",
      "5th False\n",
      ", False\n",
      "2018 False\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text3)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry == False\n",
      "I == False\n",
      "'m == False\n",
      "not == True\n",
      "free == False\n",
      "tonite == False\n",
      ", == False\n",
      "I == False\n",
      "have == True\n",
      "SA48 == True\n",
      "( == False\n",
      "lowercase == False\n",
      ": == False\n",
      "mldds == False\n",
      ") == False\n",
      ". == False\n"
     ]
    }
   ],
   "source": [
    "# Adding stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS.add('SA48')\n",
    "\n",
    "doc = nlp(u\"Sorry I'm not free tonite, I have SA48 (lowercase: mldds).\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,  '==', token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stop words with NLTK\n",
    "\n",
    "```\n",
    "nltk.corpus.stopwords\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yungh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download corpus\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flight False\n",
      "224 False\n",
      "is True\n",
      "scheduled False\n",
      "to True\n",
      "arrive False\n",
      "in True\n",
      "Frankfurt False\n",
      "at True\n",
      "4pm False\n",
      "July False\n",
      "5th False\n",
      ", False\n",
      "2018 False\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text3)\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "for token in tokens:\n",
    "    print(token, token in stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry == False\n",
      "I == False\n",
      "'m == False\n",
      "not == True\n",
      "free == False\n",
      "tonite == False\n",
      ", == False\n",
      "I == False\n",
      "have == True\n",
      "SA48 == True\n",
      "( == False\n",
      "lowercase == False\n",
      ": == False\n",
      "mldds == False\n",
      ") == False\n",
      ". == False\n"
     ]
    }
   ],
   "source": [
    "# Adding stop words\n",
    "stops = stopwords.words('english')\n",
    "stops.append(\"SA48\")\n",
    "stops = set(stops)\n",
    "\n",
    "tokens = nltk.word_tokenize(u\"Sorry I'm not free tonite, I have SA48 (lowercase: mldds).\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token,  '==', token in stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Frequency Analysis\n",
    "\n",
    "Answers two questions:\n",
    "\n",
    "1. How often does a word appear in a document?\n",
    "\n",
    "2. How important is a word in a document?\n",
    "\n",
    "Measure: Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Term Frequency\n",
    "\n",
    "Most common formula:\n",
    "\n",
    "$$\\frac{f_{t, d}}{\\sum_{t' \\in d} \\, f_{t',d}}$$\n",
    "\n",
    "$f_{t, d}$: count of term $t$ in document $d$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Inverse Document Frequency\n",
    "\n",
    "Most common formula:\n",
    "\n",
    "$$log\\frac{N}{\\mid\\{d \\in D : t \\in d \\}\\mid}$$\n",
    "\n",
    "$N$: number of documents\n",
    "\n",
    "$\\mid\\{d \\in D : t \\in d \\}\\mid$: number of documents containing term $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## TD-IDF\n",
    "\n",
    "$$tfidf(t, d, D) = tf(t, d) * idf(t, D)$$\n",
    "\n",
    "|term|tf|idf|tf-idf|\n",
    "|--|--|--|--|--|\n",
    "|to|large|very small|closer to 0|\n",
    "|coffee|small|large|not closer to 0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Computing TF-IDF\n",
    "\n",
    "#### Scikit-learn:\n",
    "\n",
    "```\n",
    "sklearn.feature_extraction.text.CountVectorizer\n",
    "\n",
    "sklearn.feature_extraction.text.TfidfVectorizer\n",
    "```\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "\n",
    "#### NLTK\n",
    "Supports tf-idf, but less popular\n",
    "```\n",
    "nltk.text.TextCollection\n",
    "```\n",
    "\n",
    "http://www.nltk.org/api/nltk.html#nltk.text.TextCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text5 = u\"This is a test.\\n\" \\\n",
    "    u\"The quick brown fox jumps over the lazy dog.\\n\" \\\n",
    "    u\"The early bird gets the worm.\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Computing Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 2, 0, 0],\n",
       "        [1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text5)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# Count word occurrences\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "\n",
    "X_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bird',\n",
       " 'brown',\n",
       " 'dog',\n",
       " 'early',\n",
       " 'fox',\n",
       " 'gets',\n",
       " 'is',\n",
       " 'jumps',\n",
       " 'lazy',\n",
       " 'over',\n",
       " 'quick',\n",
       " 'test',\n",
       " 'the',\n",
       " 'this',\n",
       " 'worm']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bird</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>early</th>\n",
       "      <th>fox</th>\n",
       "      <th>gets</th>\n",
       "      <th>is</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>test</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>worm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bird  brown  dog  early  fox  gets  is  jumps  lazy  over  quick  test  \\\n",
       "0     0      0    0      0    0     0   1      0     0     0      0     1   \n",
       "1     0      1    1      0    1     0   0      1     1     1      1     0   \n",
       "2     1      0    0      1    0     1   0      0     0     0      0     0   \n",
       "\n",
       "   the  this  worm  \n",
       "0    0     1     0  \n",
       "1    2     0     0  \n",
       "2    2     0     1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display as a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df_wc = pd.DataFrame(X_dense, columns=vectorizer.get_feature_names())\n",
    "df_wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Computing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 15)\n",
      "['bird', 'brown', 'dog', 'early', 'fox', 'gets', 'is', 'jumps', 'lazy', 'over', 'quick', 'test', 'the', 'this', 'worm']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.57735027, 0.        , 0.57735027, 0.        ],\n",
       "        [0.        , 0.32767345, 0.32767345, 0.        , 0.32767345,\n",
       "         0.        , 0.        , 0.32767345, 0.32767345, 0.32767345,\n",
       "         0.32767345, 0.        , 0.49840822, 0.        , 0.        ],\n",
       "        [0.39798027, 0.        , 0.        , 0.39798027, 0.        ,\n",
       "         0.39798027, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.60534851, 0.        , 0.39798027]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TfidfVectorizer is a combination of\n",
    "#   CountVectorizer + TfidfTransformer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "\n",
    "print(X_dense.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "X_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test.\n",
      "\n",
      "this 0.5773502691896257\n",
      "test 0.5773502691896257\n",
      "is 0.5773502691896257\n",
      "worm 0.0\n",
      "the 0.0\n",
      "quick 0.0\n",
      "over 0.0\n",
      "lazy 0.0\n",
      "jumps 0.0\n",
      "gets 0.0\n",
      "fox 0.0\n",
      "early 0.0\n",
      "dog 0.0\n",
      "brown 0.0\n",
      "bird 0.0\n",
      "\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "the 0.4984082163022241\n",
      "quick 0.3276734545947569\n",
      "over 0.3276734545947569\n",
      "lazy 0.3276734545947569\n",
      "jumps 0.3276734545947569\n",
      "fox 0.3276734545947569\n",
      "dog 0.3276734545947569\n",
      "brown 0.3276734545947569\n",
      "worm 0.0\n",
      "this 0.0\n",
      "test 0.0\n",
      "is 0.0\n",
      "gets 0.0\n",
      "early 0.0\n",
      "bird 0.0\n",
      "\n",
      "The early bird gets the worm.\n",
      "\n",
      "the 0.6053485081062917\n",
      "worm 0.3979802707840827\n",
      "gets 0.3979802707840827\n",
      "early 0.3979802707840827\n",
      "bird 0.3979802707840827\n",
      "this 0.0\n",
      "test 0.0\n",
      "quick 0.0\n",
      "over 0.0\n",
      "lazy 0.0\n",
      "jumps 0.0\n",
      "is 0.0\n",
      "fox 0.0\n",
      "dog 0.0\n",
      "brown 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for each sentence, get the highest tf-idf\n",
    "import numpy as np\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "tfidf_arr = np.array(X_dense)\n",
    "\n",
    "for i in np.arange(len(sentences)):\n",
    "    print(sentences[i])\n",
    "    sorted_idx = np.argsort(tfidf_arr[i])[::-1]\n",
    "    [print(terms[j], tfidf_arr[i][j]) for j in sorted_idx]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "1. Get 3-5 of your own sample sentences\n",
    "2. Compute the TF-IDF\n",
    "3. Compute the TF-IDF with stop_words filtered out:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "```\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=STOP_WORDS)\n",
    "\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## N-grams\n",
    "\n",
    "TF-IDF can be applied to N-grams (N words at a time), to try to capture some context information.\n",
    "\n",
    "```\n",
    "CountVectorizer(ngram_range=(minN, maxN)), ..)\n",
    "\n",
    "TfidfVectorizer(ngram_range=(minN, maxN)), ..)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text5 = u\"This is a test.\\n\" \\\n",
    "    u\"The quick brown fox jumps over the lazy dog.\\n\" \\\n",
    "    u\"The early bird gets the worm.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bird</th>\n",
       "      <th>bird gets</th>\n",
       "      <th>brown</th>\n",
       "      <th>brown fox</th>\n",
       "      <th>dog</th>\n",
       "      <th>early</th>\n",
       "      <th>early bird</th>\n",
       "      <th>fox</th>\n",
       "      <th>fox jumps</th>\n",
       "      <th>gets</th>\n",
       "      <th>...</th>\n",
       "      <th>quick brown</th>\n",
       "      <th>test</th>\n",
       "      <th>the</th>\n",
       "      <th>the early</th>\n",
       "      <th>the lazy</th>\n",
       "      <th>the quick</th>\n",
       "      <th>the worm</th>\n",
       "      <th>this</th>\n",
       "      <th>this is</th>\n",
       "      <th>worm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bird  bird gets  brown  brown fox  dog  early  early bird  fox  fox jumps  \\\n",
       "0     0          0      0          0    0      0           0    0          0   \n",
       "1     0          0      1          1    1      0           0    1          1   \n",
       "2     1          1      0          0    0      1           1    0          0   \n",
       "\n",
       "   gets  ...  quick brown  test  the  the early  the lazy  the quick  \\\n",
       "0     0  ...            0     1    0          0         0          0   \n",
       "1     0  ...            1     0    2          0         1          1   \n",
       "2     1  ...            0     0    2          1         0          0   \n",
       "\n",
       "   the worm  this  this is  worm  \n",
       "0         0     1        1     0  \n",
       "1         0     0        0     0  \n",
       "2         1     0        0     1  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text5)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# Count word occurrences using 1 and 2-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "print(X_dense.shape)\n",
    "\n",
    "pd.DataFrame(X_dense, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise: TF-IDF with Trigrams\n",
    "\n",
    "- Compute the TF-IDF for trigrams (1 to 3-grams), using your sample text.\n",
    "- Try with and without stop words included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLTK N-gram support\n",
    "\n",
    "You can also split text into trigrams and bigrams using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'a'), ('a', 'field'), ('field', 'one'), ('one', 'summer'), ('summer', \"'s\"), (\"'s\", 'day'), ('day', 'a'), ('a', 'Grasshopper'), ('Grasshopper', 'was'), ('was', 'hopping'), ('hopping', 'about'), ('about', ','), (',', 'chirping'), ('chirping', 'and'), ('and', 'singing'), ('singing', 'to'), ('to', 'its'), ('its', 'heart'), ('heart', \"'s\"), (\"'s\", 'content'), ('content', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams, trigrams, ngrams, word_tokenize\n",
    "\n",
    "# http://www.taleswithmorals.com/aesop-fable-the-ant-and-the-grasshopper.htm\n",
    "text6 = \"In a field one summer's day a Grasshopper was hopping about, \" \\\n",
    "        \"chirping and singing to its heart's content.\"\n",
    "\n",
    "words = word_tokenize(text6)\n",
    "\n",
    "print(list(bigrams(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'a', 'field'), ('a', 'field', 'one'), ('field', 'one', 'summer'), ('one', 'summer', \"'s\"), ('summer', \"'s\", 'day'), (\"'s\", 'day', 'a'), ('day', 'a', 'Grasshopper'), ('a', 'Grasshopper', 'was'), ('Grasshopper', 'was', 'hopping'), ('was', 'hopping', 'about'), ('hopping', 'about', ','), ('about', ',', 'chirping'), (',', 'chirping', 'and'), ('chirping', 'and', 'singing'), ('and', 'singing', 'to'), ('singing', 'to', 'its'), ('to', 'its', 'heart'), ('its', 'heart', \"'s\"), ('heart', \"'s\", 'content'), (\"'s\", 'content', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(list(trigrams(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'a', 'field', 'one'), ('a', 'field', 'one', 'summer'), ('field', 'one', 'summer', \"'s\"), ('one', 'summer', \"'s\", 'day'), ('summer', \"'s\", 'day', 'a'), (\"'s\", 'day', 'a', 'Grasshopper'), ('day', 'a', 'Grasshopper', 'was'), ('a', 'Grasshopper', 'was', 'hopping'), ('Grasshopper', 'was', 'hopping', 'about'), ('was', 'hopping', 'about', ','), ('hopping', 'about', ',', 'chirping'), ('about', ',', 'chirping', 'and'), (',', 'chirping', 'and', 'singing'), ('chirping', 'and', 'singing', 'to'), ('and', 'singing', 'to', 'its'), ('singing', 'to', 'its', 'heart'), ('to', 'its', 'heart', \"'s\"), ('its', 'heart', \"'s\", 'content'), ('heart', \"'s\", 'content', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Datasets\n",
    "\n",
    "http://www.nltk.org/nltk_data/\n",
    "\n",
    "https://github.com/niderhoff/nlp-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Paragraph Summarization\n",
    "\n",
    "- Download a corpus from NLTK\n",
    "- Split the corpus into paragraphs\n",
    "- Compute TF-IDF score for each word in a paragraph corresponding to its level of \"importance\"\n",
    "- Rank each sentence using (sum of TF-IDF(words) / number of tokens)\n",
    "- Extract the top N highest scoring sentences and return them as our \"summary\"\n",
    "\n",
    "Credits: https://github.com/charlieg/A-Smattering-of-NLP-in-Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a corpus\n",
    "\n",
    "Select a corpus from http://www.nltk.org/nltk_data/. \n",
    "\n",
    "Suggestions:\n",
    "- reuters\n",
    "- abc\n",
    "- gutenberg\n",
    "\n",
    "Example\n",
    "```\n",
    "# download the corpus you selected\n",
    "import nltk\n",
    "nltk.download('abc')\n",
    "\n",
    "# update the import with the corpus you selected\n",
    "from nltk.corpus import abc as corpus\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the corpus\n",
    "\n",
    "For example, try printing the raw text of one of the files:\n",
    "\n",
    "```\n",
    "fileids = corpus.fileids()\n",
    "\n",
    "print(fileids)\n",
    "\n",
    "print(corpus.raw(fileids[0])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Split the text into paragraphs\n",
    "\n",
    "NLTK doesn't include a paragraph tokenizer, so we'll try to create our own.\n",
    "\n",
    "One logic that may work is this:\n",
    "- a paragraph is detected if there are consecutive newline characters\n",
    "\n",
    "Adapt this function to your corpus, and adjust the logic if necessary to get paragraphs.\n",
    "\n",
    "```\n",
    "def tokenize_paragraph(text):\n",
    "    \"\"\"Tokenizes text into paragraphs\n",
    "    Args:\n",
    "        text - the raw text\n",
    "    Returns:\n",
    "        A list of paragraphs for the raw text\n",
    "    \"\"\"\n",
    "    # Note: you may need to customize this logic for the \n",
    "    # corpus you selected\n",
    "    return [p for p in text.split('\\n\\n') if p]\n",
    "\n",
    "# test code\n",
    "paragraphs = tokenize_paragraph(corpus.raw(fileids[0]))\n",
    "print(paragraphs[:5])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Collect all paragraphs in your corpus\n",
    "\n",
    "Using the paragraph tokenizer, create a list containing all paragraphs for all the files in the corpus.\n",
    "\n",
    "We will be using this to train TF-IDF.\n",
    "\n",
    "Some starter code:\n",
    "\n",
    "```\n",
    "all_paragraphs = []\n",
    "for fileid in corpus.fileids():\n",
    "    text = corpus.raw(fileid)\n",
    "\n",
    "    # Split text into paragraphs and add to all_paragraphs\n",
    "    # You can use the syntax list1 = list1 + list2\n",
    "    #\n",
    "    # Your code here\n",
    "    ...\n",
    "    \n",
    "\n",
    "# test code\n",
    "print(len(all_paragraphs))\n",
    "print(all_paragraphs[:5])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    \"\"\"Helper function to tokenize and stem words in text\n",
    "    Arg:\n",
    "        text: the input text\n",
    "    Return:\n",
    "        the tokenized stem words\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from nltk.stem import PorterStemmer\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compute TF-IDF\n",
    "\n",
    "- Treating a paragraph as a document, compute the TF-IDF using TfidfVectorizer\n",
    "- Pass `tokenize_and_stem` as a tokenizer to TfidfVectorizer\n",
    "- Filter out stop words in TfidfVectorizer\n",
    "- `fit_transform` the TfidfVectorizer (this may take about a minute or two)\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english')\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explore the TF-IDF matrix\n",
    "\n",
    "Explore the TF-IDF matrix, counting the terms, documents, and printing the first few terms\n",
    "\n",
    "```\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# number of terms\n",
    "print(\"Number of terms:\", len(feature_names))\n",
    "\n",
    "# number of documents (paragraphs)\n",
    "print(\"Number of paragraphs:\", tfidf.shape[0])\n",
    "\n",
    "# first 20 terms\n",
    "print(feature_names[:20])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Paragraph Summarization\n",
    "\n",
    "- Pick a random paragraph\n",
    "- Tokenize the paragraph into sentences\n",
    "- Rank each sentence by getting the average word score for it\n",
    "- Extract the top N highest scoring sentences and return them as our \"summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Get a random index for all_paragraphs\n",
    "paragraph_index = random.randint(0, len(all_paragraphs)-1)\n",
    "paragraph = all_paragraphs[paragraph_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the selected paragraph into sentences\n",
    "# for each sentence, compute the sum of TF-IDF divided by tokens\n",
    "sentence_scores = []\n",
    "for sentence in sent_tokenize(paragraph):\n",
    "    tfidf_sum = 0\n",
    "\n",
    "    sent_tokens = tokenize_and_stem(sentence)\n",
    "    feature_tokens = [t for t in sent_tokens if t in feature_names]\n",
    "\n",
    "    for ft in feature_tokens:\n",
    "        tfidf_sum += tfidf[paragraph_index, feature_names.index(ft)]\n",
    "    \n",
    "    sentence_score = tfidf_sum / len(sent_tokens)\n",
    "    sentence_scores.append((sentence_score, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top-N scores and create the summary\n",
    "\n",
    "# in case paragraph has less than 2 sentences\n",
    "n = min(2, len(sentence_scores))\n",
    "\n",
    "# sort by sentence_score\n",
    "sentence_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('*** SUMMARY ***')\n",
    "for summary_sentence in sentence_scores[:n]:\n",
    "    print(summary_sentence[1], '(score: %.2f)' % summary_sentence[0])\n",
    "\n",
    "print('\\n*** ORIGINAL ***')\n",
    "print(paragraph)\n",
    "\n",
    "print('\\n*** SENTENCE SCORES ***')\n",
    "for (score, sentence) in sentence_scores:\n",
    "    print(score, sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
